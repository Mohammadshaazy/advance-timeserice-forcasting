import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


n_steps = 1500
n_features = 5
noise_std = 0.3


t = np.arange(n_steps)


feature_1 = 0.02 * t + 5 * np.sin(0.02 * t)
feature_2 = 3 * np.sin(0.1 * t) + 0.5 * np.sin(0.03 * t + 1)
feature_3 = 0.5 * t**0.2 + 2 * np.cos(0.05 * t)
feature_4 = np.sin(0.015 * t) * np.cos(0.1 * t) * 10
feature_5 = np.random.randn(n_steps).cumsum() * 0.1 + 2 * np.sin(0.05 * t)


features = np.vstack([
    feature_1 + np.random.normal(0, noise_std, n_steps),
    feature_2 + np.random.normal(0, noise_std, n_steps),
    feature_3 + np.random.normal(0, noise_std, n_steps),
    feature_4 + np.random.normal(0, noise_std, n_steps),
    feature_5 + np.random.normal(0, noise_std, n_steps),
]).T


df = pd.DataFrame(features, columns=[f"feature_{i}" for i in range(1, 6)])
df.head()


df.plot(figsize=(14,5))
plt.title("Multivariate Time Series (5 features, 1500 timesteps)")
plt.show()



from sklearn.preprocessing import MinMaxScaler
import numpy as np


input_length = 60
forecast_horizon = 10


scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

X = []
y = []


for i in range(len(scaled_data) - input_length - forecast_horizon):
    X.append(scaled_data[i : i + input_length])
    y.append(scaled_data[i + input_length : i + input_length + forecast_horizon, 0])


X = np.array(X)
y = np.array(y)

print("X shape:", X.shape)
print("y shape:", y.shape)




import tensorflow as tf
from tensorflow.keras import layers, models

class AttentionLayer(layers.Layer):
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight",
                                 shape=(input_shape[-1], input_shape[-1]),
                                 initializer="random_normal")
        self.b = self.add_weight(name="att_bias",
                                 shape=(input_shape[-1],),
                                 initializer="zeros")
        super().build(input_shape)

    def call(self, inputs):

        e = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)


        alphas = tf.nn.softmax(e, axis=1)


        context = tf.reduce_sum(inputs * alphas, axis=1)

        return context, alphas



input_layer = layers.Input(shape=(input_length, 5))


lstm_out = layers.LSTM(64, return_sequences=True)(input_layer)


context_vector, attention_weights = AttentionLayer()(lstm_out)

output = layers.Dense(forecast_horizon)(context_vector)

model = models.Model(inputs=input_layer, outputs=output)


model.compile(optimizer="adam", loss="mse")

model.summary()





split = int(0.8 * len(X))

X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

print("Train shapes:", X_train.shape, y_train.shape)
print("Test shapes:", X_test.shape, y_test.shape)


history = model.fit(
    X_train,
    y_train,
    validation_data=(X_test, y_test),
    epochs=20,
    batch_size=32,
    verbose=1
)




sample_id = 0

X_sample = X_test[sample_id:sample_id+1]
y_true = y_test[sample_id]


y_pred = model.predict(X_sample)[0]

print("TRUE:", y_true)
print("PRED:", y_pred)



plt.figure(figsize=(10,4))
plt.plot(y_true, label="True")
plt.plot(y_pred, label="Predicted")
plt.title("Forecast: True vs Predicted (10 steps)")
plt.legend()
plt.show()



attention_model = models.Model(inputs=model.input,
                               outputs=model.layers[2].output[1])

attention_scores = attention_model.predict(X_sample)[0]
attention_scores = attention_scores.mean(axis=1)

plt.figure(figsize=(12,4))
plt.plot(attention_scores)
plt.title("Attention Weights Across 60 Input Timesteps")
plt.xlabel("Timestep")
plt.ylabel("Attention Weight")
plt.show()

